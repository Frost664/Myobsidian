## 机器学习基础

#机器学习

# 绪论

#深度学习

<mark style="background: #FFB8EBA6;">深度学习</mark>问题是一个机器学习问题，指<mark style="background: #BBFABBA6;">从有限样例中通过算法总结出一般性的规律，并可以应用到新的未知数据上。</mark>

> 深度学习 是机器学习的一个分支

深度学习的模型复杂：模式是指 样本的原始输入到输出目标之间的数据流经过多个线性/非线性的组件

贡献度的提出：组件对 最后结果的贡献
因为不清楚 每个组件对贡献对分配的多少，所以 给出了<mark style="background: #FF5582A6;">贡献度分配问题</mark>。

为了解决贡献度分配问题，有 人工神经网络（神经网络）<mark style="background: #FFB86CA6;">ANN</mark>

> 受到人脑神经系统的工作方式启发的而构造的数学模型

#人工智能

## 人工智能

人工智能的定义：人工智能就是要让机器的行为看起来就像是人类所表现出的智能行为一样。
主要领域分为，感知、学习、认知。

专家系统 基于知识的系统

#机器学习

## 机器学习

机器学习：从有限的观测数据中学习（或“猜测”）出具有一般性的规律，并利用这些规律对未知数据进行预测的方法。机器学习是人工智能的一个重要分支，并逐渐成为推动人工智能发展的关键因素．

传统的机器学习主要关注如何学习一个预测模型．一般需要首先将数据表示为一组特征（Feature），特征的表示形式可以是连续的数值、离散的符号或其他形式．然后将这些特征输入到预测模型，并输出预测结果．这类机器学习可以看作浅层学习（Shallow Learning）．浅层学习的一个重要特点是不涉及特征学习，其特征主要靠人工经验或特征转换方法来抽取．

机器学习的一般步骤：

1. 数据预处理
2. 特征提取
3. 特征转换
4. 预测
   ![[Pasted image 20231122093806.png]]
   <mark style="background: #ADCCFFA6;">主要工作在 预处理、特征提取、和特征转换</mark>上面，因为传统机器学习模型主要关注最后一步（构建预测函数），现实情况下不同的预测模型性能差异不大。但是前三步中的特征处理有关键性作用。最后 机器学习问题 变成了特征工程。

## 表示学习

#表示学习
<mark style="background: #FFB86CA6;">表示学习的定义</mark>：
为了提高机器学习系统的准确率，我们就需要将输入信息转换为有效的特征，或者更一般性地称为表示（Representation）．如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作表示学习（Representation Learning）．

> 为了去自动学习 有效的特征，然后去提供模型的性能。
> 应该还可以应用到别的 需要自主学习特征的地方

#语义鸿沟
语义鸿沟：表示学习的关键是解决语义鸿沟

> 示（即底层特征）差异性也会非常大．但是我们理解这些图片是建立在比较抽象的高层语义概念上的．如果一个预测模型直接建立在底层特征之上，会导致对预测模型的能力要求过高．如果可以有一个好的表示在某种程度上能够反映出数据的高层语义特征，那么我们就能相对容易地构建后续的机器学习模型．在表示学习中，有两个核心问题：一是“什么是一个好的表示”；二是“如何学习到好的表示”．

在机器学习中，我们经常使用两种方式来表示特征：<mark class="hltr-orange">局部表示</mark>（Local Representation）和<mark class="hltr-orange">分布式表示</mark>（Distributed Representation）
局部表示：通常用 one-hot 向量的表示.

表示学习的关键是构建具有一定深度的多层特征表示

## 深度学习

深度学习 是机器学习的一个子问题，主要目的是从数据中自动学习到有效的特征表示
![[Pasted image 20231123105115.png]]
将原始的数据特征通过多步的特征转换得到一种标识，深度学习的关键问题是贡献度的分配问题
我认为贡献度分配问题就是 一个权重的影响程度，各种参数
深度学习可以看成是一种强化学习

深度学习采用的模型主要还是 神经网络模型，其可以很好的解决贡献度分配问题，使用的是误差反向传播算法。

端到端学习 #端到端学习

> 也是端到端训练：在学习的过程中，不使用分模块或分阶段训练。
> 直接优化任务的总体目标。

训练数据为 “输入 -- 输出” 对的形式。无需提供额外信息。
端到端学习 和 深度学习一样 最后的都是要解决贡献度分配问题

## 神经网络

早期的神经科学家构造了一种模仿人脑神经系统的数学模型（人工神经网络，神经网络）

人工神经网络就是为了模拟人脑神经网络而 设计的一种计算模型。不同节点之间的连接被赋予了不同的权重，每个权重代表了一个节点对另一个节点的影响大小。 ； 每个节点代表一种特定的函数

神经网络的发展历史

## 知识体系

机器学习

> 机器学习可以分为监督学习、无监督学习和强化学习
> 三要素：模型、学习准则和优化算法
> 线性模型

神经网络
概率图模型

# 机器学习概述

早期 机器学习也被称为模式识别

**特征/属性**
**标签**
将一个 标记好**特征**与**标签**的芒果看成一个样本；也就是**示例**
一组样本构成的集合被称为**数据集**：数据集一般被分为两个部分，**训练集和测试集**（测试集用来检验模型的好坏，也叫做测试样本）

用一个 D 维向量 x 来表示芒果的所有特征构成的向量，称为**特征向量**
芒果的标签用 y 来表示。
![[Pasted image 20231125221802.png]]

    这应该是意味着：我有一个芒果，这个芒果有这些特征x（如大，甜等），我们认为他是一个好的芒果，这个“好”就是标签，我们人类的主管思想y。
    每一个 芒果的x对于一个y值，这中间有可能存在着一种 对应关系，也就是函数 f(x)。将其一一映射

---

这个函数 f(x) 我们要去找这个最优解。
找这个最优解，我们需要学习算法(Learning Algorithm)，寻找过程 称为 **学习/训练**过程

然后，将这个算法应用于，之后的买芒果中。

而如何知道这个 f\*(x)是否优秀，我们又需要一个 函数来进行评价，这个函数就是评价函数/准确率
![[Pasted image 20231125222941.png]]
![[Pasted image 20231125222948.png]]

## 机器学习的三个基本要素

### 1 模型

![[Pasted image 20231125231207.png]]
机器学习的目标是找到一个模型来近似真实的 映射函数 或者真实条件概率分布 pr(y|x)

因为我们不知道真实的映射函数 g(x)/条件概率的具体形式，只能通过经验来假设一个函数的集合 F，称为**假设空间**。（这应该表示的 一堆函数，我们认为可能近似与真实的映射函数的 函数形式）。
通过观察 其在训练集 D 上的特性，选择应该理想型的假设
![[Pasted image 20231125231819.png]]
一般的假设空间分别线性/非线性，所以模型也是线性与非线性两种。

### 线性模型

![[Pasted image 20231125232149.png]]

### 非线性模型

![[Pasted image 20231125232155.png]]![[Pasted image 20231125232201.png]]

## 学习准则

一个好的模型 应该尽可能的对 所有（x，y）的取值都与真实的映射函数一致/或与条件分布一致

### 损失函数

一个非负实数函数，用来量化模型预测和真实标签之间的差异

![[Pasted image 20231127193327.png]]
![[Pasted image 20231127193430.png]]

### 风险最小化准则

一个好的模型应当有一个比较小的期望错误，但由于不知道真实的数据 和 映射函数。实际上无法计算其期望风险。所以给定一个数据集，去计算经验风险，及也就是所说的经验风险最小化准则
**过拟合**：训练集数据趋于无穷大时，经验趋向于期望风险。但是，训练数据是不能到无穷的，是真实数据的小子集，且含有噪声。
经验最小化容易导致模型在训练集上错误率低，但是在未知的数据上错误率过高。形成：**过拟合**

**过拟合问题的原因**

> 训练数据少，噪声，和模型能力强等原因造成的

如何解决过拟合问题
一般在经验风险最小化的基础上在引入参数的**正则化**，来限制模型的能力。这个准则是**结构风险最小化**.

过拟合相反的概念是**欠拟合**，模型不能很好的拟合训练数据
欠拟合一般 是 由于模型能力不足造成的
![[Pasted image 20231127201632.png]]

总结： -机器学习中的学习准则，不仅仅是拟合训练集上的数据。同时也要使得泛化错误最低. 给定一个训练集，机器学习的目标是从假设空间中找到一个泛化错误较低的“理想“模型。（也就是找到一个通用的好的模型，来解决这一类问题）以便更好地对未知的样本进行预测，特别是 不在训练集中出现的样本。因此，可以将机器学习看作一个从有限、高维、有噪声的数据上得到跟一般性规律的泛化问题。

### 优化算法

在确定了训练集 D，假设空间 F 和学习准则后，如何找到最优的模型 f(x,θ)就成了一个**最优化**问题。
机器学习的训练过程其实就是最优化问题的求解过程

**参数与超参数** 优化可以分为，参数优化与超参数优化，模型 f(x;Θ)中的 θ 称为模型的参数，通过优化算法可以进行学习，除了可学习的参数 θ，外。还有一类参数是用来定义模型结构或优化策略的，这类参数叫做**超参数**。

超参数

> 聚类算法中的个数、梯度下降法中的步长、正则化项的系数、神经网络的层数、支持向量机中的核函数。

#### 梯度下降法

![[Pasted image 20231128112055.png]]

#### **提前停止**

针对梯度下降的优化算法，除了加正则化项之外，还可以通过提取停止来防止过拟合.

**验证集**：是除了训练集和测试集之外的数据。作用：进行模型选择，测试模型在验证集上是否最优。
在每次迭代时将，新的模型在验证集上测试，并计算错误率，如果错误率不再下降，就停止迭代，这就是提前停止。 #验证集
![[Pasted image 20231128112708.png]]

#### 随机梯度下降算法

批量梯度下降法 ：在每次迭代时要计算每个样本上的的损失函数的梯度并求和，当训练集中的样本数量 N 很大时，空间复杂度高，每次迭代的计算开销也很大。每次迭代的计算开销也大。

批量梯度下降算法，从真实数据中采集 N 个样本 并由它们计算出来的经验风险的梯度来近似期望风险的梯度。

**<mark class="hltr-red">批量梯度</mark>下降和<mark class="hltr-orange">随机梯度</mark>下降**之间的区别在于，每次迭代的优化目标是对<mark class="hltr-red">所有样本</mark>的平均损失函数还是对<mark class="hltr-orange">单个样本</mark>的损失函数．由于随机梯度下降实现简单，收敛速度也非常快，因此使用非常广泛．<mark class="hltr-grey">随机梯度下降相当于在批量梯度下降的梯度上引入了随机噪声</mark>．在非凸优化问题中，随机梯度下降<mark class="hltr-yellow">更容易逃离局部最优点</mark>．

#### 小批量梯度下降法

随机梯度下降法的一个<mark class="hltr-red">缺点</mark>是无法充分利用计算机的<mark class="hltr-yellow">并行计算能力</mark>．**小批量梯度下降法**（Mini-Batch Gradient Descent）是批量梯度下降和随机梯度下降的**折中**．每次迭代时，我们随机选取一小部分训练样本来计算梯度并更新参数，这样既可以兼顾随机梯度下降法的优点，也可以提高训练效率．

在实际应用中，<mark class="hltr-green">小批量随机梯度下降法</mark>有收敛快、计算开销小的优点，因此

逐渐成为大规模的机器学习中的主要优化算法

## 机器学习的简单实例 ——线性回归

线性回归是机器学习与统计学中最基础和最广泛应用的模型，是一种对自变量和因变量之间关系进行建模的回归分析。自变量数量为 1 时，称为**简单回归**，自变量数量大于 1 时称为**多元回归**。
![[Pasted image 20231128151253.png]]
![[Pasted image 20231128151307.png]]
W 和 x 分别表示增广权重向量和增广特征向量。线性回归模型简写为
$$f(x;w)=w^Tx$$

### 参数学习

#### 经验风险最小化

#### 结构风险最小化

#### 最大似然估计

## 偏差-方差分解

如何在模型的拟合能力和复杂度之间取得一个较好的平衡，对一个机器学习算法来讲十分重要，**偏差-方差分解**提供一个分析指导工具。
![[Pasted image 20231128160629.png]]
<mark class="hltr-purple">方差一般会随着训练样本的增加而减少</mark>．当样本比较多时，方差比较少，这时可以选择能力强的模型来减少偏差．然而在很多机器学习任务上，训练集往往都比较有限，<mark class="hltr-purple">最优的偏差和最优的方差就无法兼顾</mark>．

随着模型复杂度的增加，模型的拟合能力变强，偏差减少而方差增大，从而导致过拟合．以结构风险最小化为例，我们可以<mark class="hltr-cyan">调整正则化系数 𝜆 来控制模型的复杂度</mark>．当 𝜆 变大时，模型复杂度会降低，可以有效地减少方差，避免过拟合，但偏差会上升．当 𝜆 过大时，总的期望错误反而会上升．因此，一个好的正则化系数 𝜆 需要在偏差和方差之间取得比较好的平衡．图 2.7 给出了机器学习模型的期望错误、偏差和方差随复杂度的变化情况，其中<mark class="hltr-red">红色虚线表示最优模型</mark>．最优模型并不一定是偏差曲线和方差曲线的交点．
![[Pasted image 20231128160843.png]]

偏差和方差分解给机器学习模型提供了一种分析途径，但在实际操作中难以直接衡量．一般来说，当一个模型在训练集上的错误率比较高时，说明模型的拟合能力不够，偏差比较高．这种情况可以通过增加数据特征、提高模型复杂度、减小正则化系数等操作来改进．

当模型在训练集上的错误率比较低，但验证集上的错误率比较高时，说明模型过拟合，方差比较高．这种情况可以通过降低模型复杂度、加大正则化系数、引入先验等方法来缓解．此外，还有一种有效降低方差的方法为**集成模型**，即<mark class="hltr-yellow">通过多个高方差模型的平均来降低方差</mark>．

## 机器学习算法的类型

监督学习
目标是建模样本的特征 x 和标签 y 之间的关系：y=f(x;θ)/离散表达，并且训练集中每个样本都有标签，那么这类机器学习称为监督学习

根据标签类型的不同，监督学习又可以分为回归问题、分类问题和结构化学习问题。

1. 回归问题（Regression)问题中的标签 y 是连续值，f 的输出也是连续值
2. 分类问题（classification）中 y 是离线的，学习到的模型也被称为 **分类器**，依据其类别数量又可分为**二分类**和**多分类**。
3. 结构化学习（structured Learning）问题，是一种特殊的分类问题。标签 y 是结构化的对象，比如序列、树、或图等。由于结构化学习的输出空间比较大，因此我们一般定义一个联合特征空间，将 x，y 映射为该空间中的联合特征向量
   ![[Pasted image 20231128162139.png]]
   无监督学习
   是指从<mark class="hltr-orange">不包含目标标签</mark>的训练样本中自动学习到一些有价值的信息．典型的无监督学习问题有<mark class="hltr-orange">聚类、密度估计、特征学习、降维</mark>等

强化学习
通过交互来学习的机器学习算法．在强化学习中，智能体根据环境的状态做出一个动作，并<mark class="hltr-orange">得到即时或延时的奖励</mark>．智能体在和环境的交互中不断<mark class="hltr-orange">学习并调整策略</mark>，以取得最大化的期望总回报

<mark class="hltr-grey">监督学习需要每个样本都有标签，而无监督学习则不需要标签．一般而言，监督学习通常需要大量的有标签数据集，这些数据集一般都需要由人工进行标注，成本很高</mark>．因此，也出现了很多**弱监督学习**（Weakly Supervised Learning）和**半监督学习**（Semi-Supervised Learning，SSL）的方法，希望从大规模的无标注数据中充分挖掘有用的信息，降低对标注样本数量的要求．强化学习和监督学习的不同在于，强化学习不需要显式地以“输入/输出对”的方式给出训练样本，是一种在线的学习机制

![[Pasted image 20231128162347.png]]

## 数据的特征表示

**特征抽取**（Feature Extraction）是构造一个新的特征空间，并将原始特征投影在新的空间中得到新的表示．

**特征选择**（Feature Selection）是选取原始特征集合的一个有效子集，使得基于这个特征子集训练出来的模型准确率最高．简单地说，<mark class="hltr-orange">特征选择就是保留有用特征，移除冗余或无关的特征</mark>

## 理论和定理

### 2.8.1 PAC 学习理论

使用机器学习方法来解决某个特定问题的时候，通过靠经验或多次试探来选择合适的模型，训练样本数量以及学习算法收敛的速度等。
由于采用经验判断或多次试验往往成本比较高，也不可靠，因此 希望可以有一套理论分析问题难度、计算模型能力，为学习算法提供理论保证，并指导机器学习模型和学习算法的设计。

<mark class="hltr-pink">这就是 计算学习理论 计算学习理论（Computational Learning Theory）是机器学习理论的基础，其中最基础的理论就是可能近似性正确（Probably Approximately Correct，PAC）学习理论</mark>

机器学习中一个很关键的问题是期望错误和经验错误之间的差异，称为<mark class="hltr-orange">泛化错误</mark>。它被用来去衡量一个机器学习模型 f 是否可以很好的泛化到未知数据。

PAC 学习：主要指 学习算法可以以一定的概率学习到一个近似正确的假设，一个 PAC 可学习的算法是指该学习算法能够在多项式时间内从合理数量的训练数据中学习到一个近似正确的 f(x) 映射函数

PAC 学习 分为两个部分

1. 近似正确：它的泛化错误 ＜一个界限 α（一般在 0 到 1/2 之间）。如果他的泛化错误较大则不能用来作为正确的预测
2. 可能：一个算法可能以 一个 一般为 1/2< <1 的概率来学习到这样一个近似假设

![[Pasted image 20231210201303.png]]

### 2.8.2 没有免费午餐定理

记个结论就行
没有免费午餐定理对于机器学习算法也同样适用．不存在一种机器学习算法适合于任何领域或任务．如果有人宣称自己的模型在所有问题上都好于其他模型，那么他肯定是在吹牛．

所以！写论文的时候不能 把自己说的太 nb 了

### 2.8.3 奥卡姆剃刀原理

奥卡姆剃刀原理主要是由 一个 14 世纪的逻辑学家提出的一个解决问题的法则：如无必要，勿增实体，
这与机器学习中的正则化思想类似：简单的模型泛化性更好。

模型越简单对于，解决各类问题的效果就越好，而不会指面向于一小类。

奥卡姆剃刀的一种形式化是最小描述长度原则，最好的模型会使得数据集的压缩效果最好，即编码长度最小。

### 丑小鸭定理

丑小鸭定理（Ugly Duckling Theorem）是 1969 年由渡边慧提出的[Watanabe, 1969]．
这里的“丑小鸭”是指
白天鹅的幼雏，而不是
“丑陋的小鸭子”．
渡边慧（1910 ～ 1993），
美籍日本学者，理论物
理学家，也是模式识别
的最早研究者之一．

“丑小鸭与白天鹅之间的区别和两只白天鹅之间的区别一样大”．这
个定理初看好像不符合常识，但是仔细思考后是非常有道理的．因为世界上不存在相似性的客观标准，一切相似性的标准都是主观的．如果从体型大小或外貌的角度来看，丑小鸭和白天鹅的区别大于两只白天鹅的区别；但是如果从基因的角度来看，丑小鸭与它父母的差别要小于它父母和其他白天鹅之间的差别。

### 归纳偏置

在机器学习中，很多学习算法经常会对学习的问题做一些假设，这些假设就称为归纳偏置（Inductive Bias）[Mitchell, 1997]．比如在最近邻分类器中，我们会假设在特征空间中，一个小的局部区域中的大部分样本同属一类．在朴素贝叶斯分类器中，我们会假设每个特征的条件概率是互相独立的．
归纳偏置在贝叶斯学习中也经常称为先验（Prior）．


