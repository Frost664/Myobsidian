
# 什么是图
介绍图的概念

图形表示实体（_节点_）集合之间的关系（_边_）。

# 什么是图神经网络

当然，图 也可以表示图像，通常 将图像看出一个 有[[图像通道]]的矩形网格，常用数组表示

另一种情况是将图像看作有规则结构的图形，每一个像素点代表一个节点，通过边e来连接到相邻的像素。而每一个非边界像素 如图中的2-2一样有8个相邻的像素。存在每个节点上的信息则是表示像素RGB值的3维向量。

这里就是有一个简单的5 * 5 的图像 用图结构来表示的

此外图也可以用来表示文本之间的联系

在显示生活中 图还可以用来表示化学分子，分子他在空间上 上三维的。其中节点表示原子，存储原子的信息，边表示共价键，存储共价键的信息。

当然图也可以用来表示 社交网络，在一个社交网络中 用节点表示人，而人之间的关系则用边来表示

这些图结构的数据可以用来做什么呢   图上的预测任务一般有3种类型：图级别，点级别，边级别

在图级别任务中，预测整个图形的单个属性。对于节级别任务，预测图中每个节点的一些属性。对于边级任务，希望预测图形中边的属性或存在。

在图形级任务中，目标是预测整个图形的属性。
例如，对于表示为图形的分子，我们可能想要预测该分子闻起来是什么味道，或者它是否会与与疾病有关的受体结合。

这里输入了4个图，输出将这4个图分成了 两类

消息传递神经网络（Message Passing Neural Network，简称MPNN）是一类用于处理图结构数据的神经网络框架。MPNN主要用于学习图中节点之间的关系，通过迭代消息传递的方式来更新节点的表示。

MPNN的一般框架包括以下几个关键组件：

1. **消息传递（Message Passing）：** 在每一层中，节点会从其邻居节点接收消息，这些消息会更新节点的表示。消息通常是邻居节点的表示经过某种变换得到的。

2. **聚合（Aggregation）：** 节点接收到的消息需要进行聚合，以整合邻居节点传递过来的信息。常见的聚合方式包括求和、求平均等。

3. **更新（Update）：** 使用聚合后的信息来更新节点的表示。这个更新操作可以是一个神经网络层的输出，也可以是其他复杂的变换。

4. **池化（Pooling）：** 在图级别的任务中，可能需要对所有节点的表示进行池化操作，以获得整个图的表示。

MPNN的设计使其适用于各种图结构数据，例如社交网络、分子结构、推荐系统中的用户-物品关系等。以下是一个简单的伪代码示例，展示了MPNN的基本结构：

```python
for epoch in range(num_epochs):
    # Message Passing
    messages = message_function(graph, node_features, edge_features)
    
    # Aggregation
    aggregated_messages = aggregation_function(messages)
    
    # Update
    updated_node_features = update_function(node_features, aggregated_messages)
    
    # Optional: Pooling for graph-level tasks
    graph_representation = pooling_function(updated_node_features)
    
    # Model training or downstream tasks using updated_node_features or graph_representation
    # ...
```

不同的应用可能会使用不同的消息传递、聚合和更新函数，具体的设计取决于任务和图结构的特点。 MPNN已经在各种领域中取得了显著的成就，成为处理图数据的重要工具之一。


MLP通常指的是多层感知机（Multilayer Perceptron），这是一种常见的神经网络结构，也是深度学习中最基本和常见的结构之一。

### 多层感知机（MLP）的基本结构：

1. **输入层（Input Layer）：** 接收原始输入特征的层。

2. **隐藏层（Hidden Layer）：** 位于输入层和输出层之间的一层或多层。每个隐藏层包含多个神经元（或节点），每个神经元与前一层和后一层的神经元相连接。

3. **输出层（Output Layer）：** 产生模型的输出的层。输出层的神经元数量通常取决于任务的类型，例如，对于二分类问题可能是1个神经元，对于多分类问题可能是多个神经元。

4. **激活函数（Activation Function）：** 在每个隐藏层的神经元上应用的非线性函数，以引入非线性性质，使得模型可以学习复杂的映射关系。常见的激活函数包括ReLU（Rectified Linear Unit）、Sigmoid和tanh等。

5. **权重（Weights）和偏置（Biases）：** 每个连接都有一个相关的权重，而每个神经元都有一个偏置。这些参数通过训练过程中的反向传播来学习。

### MLP 的训练过程：

1. **前向传播（Forward Propagation）：** 输入数据通过网络，由输入层传递到输出层，形成模型的预测。

2. **损失函数计算（Loss Calculation）：** 计算模型预测与实际标签之间的差距，作为模型性能的度量。

3. **反向传播（Backward Propagation）：** 使用梯度下降等优化算法，通过链式法则计算损失函数对每个参数的梯度，然后更新参数以最小化损失。

4. **迭代（Iteration）：** 重复上述步骤，直到模型收敛或达到预定的训练迭代次数。

多层感知机在各种任务中被广泛应用，包括图像分类、语音识别、自然语言处理等。


在消息传递神经网络（Message Passing Neural Network，MPNN）中，通常没有像卷积神经网络（CNN）中的池化层那样直接的池化操作。MPNN框架更侧重于图结构数据的消息传递和节点更新，而不像图像数据那样使用池化层。

然而，MPNN中常见的一种操作是节点聚合（Node Aggregation），这可以被视为一种类似于池化的操作，尽管它的目的和实现方式可能不同。在MPNN中，节点聚合的过程是在每个节点上收集并整合邻居节点的信息，然后使用这些信息来更新节点的表示。

具体来说，节点聚合的步骤可能包括：

1. **消息传递（Message Passing）：** 对每个节点，从其邻居节点接收消息，这些消息是邻居节点表示的函数。

2. **聚合（Aggregation）：** 将从邻居节点接收到的消息进行聚合，例如通过求和或平均等方式。这一步类似于CNN中池化层的操作，但目标是整合邻居节点的信息而不是降低尺寸。

3. **更新（Update）：** 使用聚合后的信息来更新节点的表示，使节点能够捕捉其邻居节点的信息。

MPNN中的这种聚合过程有助于节点在图结构中传递和整合信息，以更好地表示节点之间的关系。尽管这不是传统意义上的图像池化，但它在MPNN框架中扮演了类似的整合邻居信息的角色，有助于学习图结构中的复杂关系。


MNIST（Modified National Institute of Standards and Technology）和CIFAR（Canadian Institute for Advanced Research）都是常用于图像分类问题的标准数据集。它们通常用于测试和比较不同机器学习和深度学习算法的性能。

1. **MNIST数据集：**
   - **类型：** MNIST数据集包含手写数字的灰度图像，每个图像都是28x28像素的大小。
   - **类别：** 数据集中的每个图像代表一个手写数字（0到9之间的数字），因此有10个类别。
   - **应用：** 通常用于入门级的图像分类问题，尤其是对于数字识别。因为图像相对简单，很多基础的机器学习和深度学习模型可以在这个数据集上进行训练和测试。

2. **CIFAR数据集：**
   - **类型：** CIFAR数据集有两个版本，其中CIFAR-10包含10个不同类别的彩色图像，而CIFAR-100包含100个不同类别的图像。
   - **类别：** CIFAR-10包含飞机、汽车、鸟类、猫、鹿、狗、青蛙、马、船和卡车等10个类别。CIFAR-100则更加细粒度，包含更多类别。
   - **应用：** 用于测试对于更复杂、多样化图像的分类算法。由于图像是彩色的，而且包含了各种不同的物体，这个数据集更具挑战性，适用于更高级的图像分类任务。

对于这两个数据集，常见的深度学习模型，如卷积神经网络（CNN），在图像分类任务上表现良好。研究人员和学生通常使用这些数据集来验证新的图像分类算法的性能，并且它们也成为了评估模型泛化能力和鲁棒性的标准基准。